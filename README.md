# <img src="./figs/vasevqa3d_logo.png" alt="logo" width="40"/> VaseVQA-3D: Benchmarking 3D VLMs on Ancient Greek Pottery

This is the official repository for the paper:
> **VaseVQA-3D: Benchmarking 3D VLMs on Ancient Greek Pottery**
>
> [Nonghai Zhang](https://github.com/sleepyDogseasea)\*, [Zeyu Zhang](https://steve-zeyu-zhang.github.io/)\*, [Jiazi Wang](https://github.com/wangjiazi), [Yang Zhao](https://github.com/zhaoyang), and [Hao Tang](https://ha0tang.github.io/)<sup>#</sup>
>
> \*Equal contribution. <sup>#</sup>Corresponding author.
>
> ### [Paper](https://arxiv.org/abs/2510.04479) | [Website](https://aigeeksgroup.github.io/VaseVQA-3D/) | [Dataset](https://huggingface.co/datasets/AIGeeksGroup/VaseVQA-3D) | [Models](https://huggingface.co/AIGeeksGroup/VaseVLM) | [HF Paper](https://huggingface.co/papers/2510.04479)

> [!NOTE]
> ğŸ’ª This visualization demonstrates VaseVQA-3D's capability in understanding and analyzing ancient Greek pottery from multiple perspectives, showcasing state-of-the-art performance in cultural heritage 3D vision-language tasks.

https://github.com/user-attachments/assets/6489a42f-1610-40a1-b2db-0bfc84029a8f

## âœï¸ Citation
If you find our code or paper helpful, please consider starring â­ us and citing:
```bibtex
@article{zhang2025vasevqa,
  title={VaseVQA-3D: Benchmarking 3D VLMs on Ancient Greek Pottery},
  author={Zhang, Nonghai and Zhang, Zeyu and Wang, Jiazi and Zhao, Yang and Tang, Hao},
  journal={arXiv preprint arXiv:2510.04479},
  year={2025}
}
```
---

## ğŸº Introduction to VaseVQA-3D

VaseVQA-3D is a comprehensive benchmark for evaluating 3D Vision-Language Models (VLMs) on ancient Greek pottery understanding. This project addresses the unique challenges of cultural heritage analysis by providing:

- **High-quality 3D Models**: A curated dataset of ancient Greek vases with detailed 3D reconstructions
- **Multi-view Analysis**: Comprehensive evaluation from multiple perspectives (front, back, left, right, top, bottom)
- **Specialized Tasks**: Question answering, captioning, and visual grounding tailored for archaeological artifacts
- **VaseVLM**: A fine-tuned vision-language model specifically designed for ancient pottery analysis

Ancient Greek pottery represents a crucial aspect of cultural heritage, containing rich historical and artistic information. However, existing 3D VLMs struggle with the specialized knowledge and visual understanding required for archaeological analysis. VaseVQA-3D bridges this gap by providing a standardized benchmark and specialized models for this domain.

![image](./figs/overview.png)

## ğŸ“° News

<b>2025/10/07:</b> ğŸ‰ Our paper has been released on arXiv.

<b>2025/10/07:</b> ğŸ“Œ Dataset and models are now available on HuggingFace.

<b>2025/10/07:</b> ğŸ”” Project website is live!

## ğŸ“‹ TODO List

> [!IMPORTANT]
> We are actively developing and improving VaseVQA-3D. Stay tuned for updates!

- [x] Upload our paper to arXiv and build project pages
- [x] Release VaseVQA-3D dataset
- [x] Release VaseVLM models
- [x] Upload training and evaluation code
- [x] Release data filtering and preprocessing scripts
- [ ] Add interactive demo on HuggingFace Spaces
- [ ] Release visualization tools
- [ ] Provide pre-trained checkpoints for all model variants

## ğŸ“ Repository Structure

```
VaseVQA-3D/
â”œâ”€â”€ 3dGenerate/          # 3D model generation from 2D images
â”œâ”€â”€ Train/               # Training scripts and data filtering
â”‚   â”œâ”€â”€ filter/         # Image quality filtering (ResNet50, CLIP)
â”‚   â””â”€â”€ model/          # Model training (SFT, GRPO, LoRA)
â”œâ”€â”€ eval/                # Evaluation scripts
â”‚   â”œâ”€â”€ qwen.py         # Qwen2.5-VL caption generation
â”‚   â”œâ”€â”€ internvl.py     # InternVL caption generation
â”‚   â”œâ”€â”€ compare.py      # Caption evaluation metrics
â”‚   â””â”€â”€ compare.sh      # Batch evaluation
â”œâ”€â”€ figs/                # Figures and visualizations
â””â”€â”€ README.md            # This file
```

## âš¡ Quick Start

### Environment Setup

Our code is tested with CUDA 11.8 and Python 3.10. To run the codes, you should first install the required packages:

```bash
# Create conda environment
conda create -n vasevqa python=3.10
conda activate vasevqa

# Install PyTorch
pip install torch==2.0.1 torchvision==0.15.0 --index-url https://download.pytorch.org/whl/cu118

# Install other dependencies
pip install transformers>=4.35.0
pip install accelerate>=0.24.0
pip install ms-swift>=2.0.0
pip install sentence-transformers
pip install lmdeploy
pip install modelscope
pip install qwen-vl-utils
```

For detailed environment setup, please refer to:
- [Train/README.md](./Train/README.md) for training environment
- [eval/README.md](./eval/README.md) for evaluation environment

### Data Preparation

#### Download VaseVQA-3D Dataset

You can download the dataset from [HuggingFace](https://huggingface.co/datasets/AIGeeksGroup/VaseVQA-3D):

```bash
# Using huggingface-cli
huggingface-cli download AIGeeksGroup/VaseVQA-3D --repo-type dataset --local-dir ./data/VaseVQA-3D
```

The dataset includes:
- **3D Models**: GLB format with textures
- **Multi-view Images**: 6 perspectives for each vase
- **Annotations**: Captions, questions, and answers
- **Metadata**: Historical information and provenance

#### Dataset Structure

```
VaseVQA-3D/
â”œâ”€â”€ models/              # 3D GLB models
â”œâ”€â”€ images/              # Multi-view rendered images
â”‚   â”œâ”€â”€ front/
â”‚   â”œâ”€â”€ back/
â”‚   â”œâ”€â”€ left/
â”‚   â”œâ”€â”€ right/
â”‚   â”œâ”€â”€ top/
â”‚   â””â”€â”€ bottom/
â”œâ”€â”€ annotations/
â”‚   â”œâ”€â”€ captions.json    # Image captions
â”‚   â”œâ”€â”€ qa_pairs.json    # Question-answer pairs
â”‚   â””â”€â”€ metadata.json    # Historical metadata
â””â”€â”€ README.md
```

#### Download Pre-trained Models

Download VaseVLM checkpoints from [HuggingFace](https://huggingface.co/AIGeeksGroup/VaseVLM):

```bash
# Download VaseVLM-3B
huggingface-cli download AIGeeksGroup/VaseVLM --repo-type model --local-dir ./models/VaseVLM-3B

# Download VaseVLM-7B
huggingface-cli download AIGeeksGroup/VaseVLM-7B --repo-type model --local-dir ./models/VaseVLM-7B
```

## ğŸ”§ 3D Model Generation

We provide tools to generate 3D models from 2D images using TripoSG:

```bash
cd 3dGenerate

# Activate environment
source env/bin/activate

# Generate 3D models
./triposg.sh assets/image/
```

For detailed instructions, see [3dGenerate/README.md](./3dGenerate/README.md).

## ğŸ’» Training

### Data Filtering

Before training, filter high-quality images using our filtering pipeline:

```bash
cd Train/filter

# Step 1: ResNet50 quality classification
python classifier.py

# Step 2: CLIP-based quality filtering
./clipfilter1.sh

# Step 3: Best view selection
python clipfilter2.py --input_dir ./filtered_vases/accepted \
                      --output_dir ./filtered_vases
```

For detailed filtering instructions, see [Train/README.md](./Train/README.md).

### Model Training

#### Supervised Fine-tuning (SFT)

```bash
cd Train/model

# Train with Qwen2.5-VL-7B
./sft.sh
```

#### GRPO Reinforcement Learning

```bash
# After SFT, perform GRPO training
./grpo.sh
```

#### Merge LoRA Weights

```bash
# Merge LoRA weights into base model
./merge.sh
```

For detailed training instructions and hyperparameters, see [Train/README.md](./Train/README.md).

## ğŸ“Š Evaluation

### Caption Generation

Generate captions using VaseVLM or other models:

```bash
cd eval

# Using Qwen2.5-VL
python qwen.py --input_dir ./data/multiview_images \
               --output_dir ./data/captions \
               --model_path ./models/VaseVLM-7B

# Using InternVL
python internvl.py --input_dir ./data/multiview_images \
                   --output_dir ./data/captions \
                   --model_path ./models/InternVL3_5-4B
```

### Evaluation Metrics

Evaluate generated captions against ground truth:

```bash
# Single model evaluation
python compare.py --generated ./data/captions/image_vasevlm.json \
                  --ground_truth ./data/groundTruth.json

# Batch evaluation
./compare.sh
```

Evaluation metrics include:
- **CLIP Score**: Semantic similarity in CLIP embedding space
- **FID Score**: Distribution similarity
- **R-Precision**: Retrieval accuracy (R@1, R@5, R@10)
- **Lexical Similarity**: Word overlap (Jaccard)
- **Overall Score**: Weighted combination of all metrics

For detailed evaluation instructions, see [eval/README.md](./eval/README.md).

## ğŸ“ˆ Benchmark Results

### Caption Generation Performance

We evaluate various models on the VaseVQA-3D benchmark. Lower FID scores and higher values for other metrics indicate better performance.

#### 3D-Specialized Models

| Method | FIDâ†“ | CLIPâ†‘ | R@10â†‘ | R@5â†‘ | R@1â†‘ | Lexical Sim.â†‘ |
|--------|------|-------|-------|------|------|---------------|
| DiffuRank | 0.421 | **0.798** | 16.67% | 8.33% | 2.08% | 0.274 |
| Cap3D | 0.445 | 0.792 | 14.58% | 7.29% | 1.56% | 0.267 |
| LLaVA3D | 0.494 | 0.784 | 10.42% | 5.21% | 1.04% | 0.238 |

#### Closed-source VLMs

| Method | FIDâ†“ | CLIPâ†‘ | R@10â†‘ | R@5â†‘ | R@1â†‘ | Lexical Sim.â†‘ |
|--------|------|-------|-------|------|------|---------------|
| Gemini-2.5-flash | **0.325** | 0.736 | **28.57%** | **17.58%** | 2.20% | 0.210 |
| Claude-4-sonnet | 0.353 | 0.676 | 23.96% | 10.42% | 3.12% | 0.188 |
| Gemini-2.5-Pro | 0.397 | 0.680 | 22.92% | 14.58% | 3.12% | 0.162 |
| GPT-4.1 | 0.501 | 0.644 | 25.00% | 10.42% | 3.12% | 0.128 |
| Claude-3.5-sonnet | 0.455 | 0.643 | 15.62% | 8.33% | 2.08% | 0.116 |
| Doubao-1.5-vision-pro-32k | 0.504 | 0.606 | 14.58% | 4.17% | 1.04% | 0.074 |
| GPT-4o | 0.582 | 0.520 | 13.54% | 6.25% | 2.08% | 0.104 |
| Claude-3.7-sonnet | 0.600 | 0.339 | 13.54% | 6.25% | 1.04% | 0.101 |

#### Open-source VLMs

| Method | FIDâ†“ | CLIPâ†‘ | R@10â†‘ | R@5â†‘ | R@1â†‘ | Lexical Sim.â†‘ |
|--------|------|-------|-------|------|------|---------------|
| InternVL | 0.376 | 0.771 | 10.42% | 8.33% | 2.08% | 0.252 |
| Qwen2.5-VL-7B | 0.334 | 0.775 | 18.75% | 9.38% | 2.08% | 0.217 |
| Qwen2.5-VL-3B | 0.358 | 0.782 | 9.38% | 6.25% | 1.04% | 0.259 |
| VaseVL | 0.493 | 0.790 | 10.4% | 6.25% | 2.08% | 0.255 |

#### Our Models

| Method | FIDâ†“ | CLIPâ†‘ | R@10â†‘ | R@5â†‘ | R@1â†‘ | Lexical Sim.â†‘ |
|--------|------|-------|-------|------|------|---------------|
| **VaseVLM-3B-SFT** | 0.359 | 0.788 | 17.71% | 8.33% | 2.08% | 0.223 |
| **VaseVLM-3B-RL** | 0.363 | 0.789 | 17.71% | 10.42% | 2.08% | 0.245 |
| **VaseVLM-7B-SFT** | 0.332 | 0.779 | 20.83% | 10.42% | 3.12% | 0.272 |
| **VaseVLM-7B-RL** | 0.328 | 0.792 | 21.24% | 11.12% | **3.52%** | **0.276** |

> **Note**: Our VaseVLM-7B-RL model achieves the best performance among open-source models on R@1 and Lexical Similarity metrics, demonstrating the effectiveness of reinforcement learning fine-tuning for cultural heritage understanding.

## ğŸ¯ Use Cases

VaseVQA-3D and VaseVLM can be applied to various cultural heritage tasks:

### 1. Archaeological Documentation
- Automated cataloging of pottery collections
- Generating detailed descriptions for museum databases
- Cross-referencing similar artifacts

### 2. Educational Applications
- Interactive learning tools for art history students
- Virtual museum guides
- Automated quiz generation

### 3. Research Support
- Pattern recognition across pottery styles
- Dating and provenance analysis
- Iconographic studies

### 4. Conservation
- Damage assessment and documentation
- Restoration planning
- Condition monitoring over time

## ğŸŒŸ Star History

[![Star History Chart](https://api.star-history.com/svg?repos=AIGeeksGroup/VaseVQA-3D&type=Date)](https://www.star-history.com/#AIGeeksGroup/VaseVQA-3D&Date)

## ğŸ¤ Contributing

We welcome contributions to VaseVQA-3D! Please feel free to:
- Report bugs and issues
- Submit pull requests
- Suggest new features
- Share your results and applications

## ğŸ“„ License

This project is released under the MIT License. See [LICENSE](./LICENSE) for details.

## ğŸ˜˜ Acknowledgement

We thank the authors of the following projects for their open-source contributions:
- [Qwen](https://github.com/QwenLM/Qwen) for the base vision-language model
- [MS-SWIFT](https://github.com/modelscope/swift) for the training framework
- [InternVL](https://github.com/OpenGVLab/InternVL) for multi-modal understanding
- [CLIP](https://github.com/openai/CLIP) for vision-language alignment
- [TripoSG](https://github.com/VAST-AI-Research/TripoSR) for 3D generation
- The museums and institutions that provided the pottery images

Special thanks to the archaeological and art history communities for their valuable feedback and domain expertise.

## ğŸ“§ Contact

For questions and discussions, please:
- Open an issue on GitHub
- Contact the authors via email
- Visit our [project website](https://aigeeksgroup.github.io/VaseVQA-3D/)

---

**Made with â¤ï¸ by the AI Geeks Group**

