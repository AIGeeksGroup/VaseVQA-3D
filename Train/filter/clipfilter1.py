#!/usr/bin/env python3

import os
import shutil
import torch
from PIL import Image
import numpy as np
from pathlib import Path
import json
from tqdm import tqdm
import argparse
import time
from transformers import CLIPProcessor, CLIPModel

class ServerVaseImageFilter:
    def __init__(self, device=None, model_cache_dir=None):
        """
        åˆå§‹åŒ–CLIPæ¨¡å‹ç”¨äºå›¾åƒè´¨é‡è¿‡æ»¤

        Args:
            device: æŒ‡å®šè®¾å¤‡ (cuda/cpu)
            model_cache_dir: è‡ªå®šä¹‰æ¨¡å‹ç¼“å­˜ç›®å½•
        """
        # è®¾å¤‡é€‰æ‹©é€»è¾‘
        if device:
            if device.startswith("cuda:"):
                # æŒ‡å®šç‰¹å®šGPU
                gpu_id = device.split(":")[1]
                if torch.cuda.is_available() and int(gpu_id) < torch.cuda.device_count():
                    self.device = device
                else:
                    print(f"âš ï¸  GPU {gpu_id} ä¸å¯ç”¨ï¼Œä½¿ç”¨é»˜è®¤è®¾å¤‡")
                    self.device = "cuda" if torch.cuda.is_available() else "cpu"
            elif device == "cuda":
                self.device = "cuda" if torch.cuda.is_available() else "cpu"
            else:
                self.device = device
        else:
            self.device = "cuda" if torch.cuda.is_available() else "cpu"

        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        if self.device.startswith("cuda"):
            print(f"GPUä¿¡æ¯: {torch.cuda.get_device_name(self.device)}")
            print(f"GPUå†…å­˜: {torch.cuda.get_device_properties(self.device).total_memory / 1024**3:.1f} GB")

        # è®¾ç½®è‡ªå®šä¹‰ç¼“å­˜ç›®å½•
        if model_cache_dir:
            self.setup_custom_cache(model_cache_dir)

        # åŠ è½½CLIPæ¨¡å‹
        print("æ­£åœ¨åŠ è½½CLIPæ¨¡å‹...")
        try:
            # æœ¬åœ°æ¨¡å‹è·¯å¾„
            local_model_path = "./openai/clip-vit-base-patch32"

            # å°è¯•çš„æ¨¡å‹è·¯å¾„å’Œåç§°ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰
            model_sources = [
                {"path": local_model_path, "name": "æœ¬åœ°æ¨¡å‹", "local_only": True},
                {"path": "openai/clip-vit-base-patch32", "name": "HuggingFace Hub", "local_only": False},
                {"path": "laion/CLIP-ViT-B-32-laion2B-s34B-b79K", "name": "LAIONæ¨¡å‹", "local_only": False},
            ]

            model_loaded = False
            for source in model_sources:
                try:
                    print(f"å°è¯•åŠ è½½æ¨¡å‹: {source['name']} ({source['path']})")

                    # æ ¹æ®æ˜¯å¦ä¸ºæœ¬åœ°æ¨¡å‹è®¾ç½®å‚æ•°
                    if source['local_only']:
                        # æ£€æŸ¥æœ¬åœ°è·¯å¾„æ˜¯å¦å­˜åœ¨
                        if not os.path.exists(source['path']):
                            print(f"âŒ æœ¬åœ°æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {source['path']}")
                            continue

                        self.model = CLIPModel.from_pretrained(
                            source['path'],
                            local_files_only=True,
                            trust_remote_code=True
                        ).to(self.device)
                        self.processor = CLIPProcessor.from_pretrained(
                            source['path'],
                            local_files_only=True,
                            trust_remote_code=True
                        )
                    else:
                        # ä»HuggingFace Hubä¸‹è½½
                        cache_dir = model_cache_dir if model_cache_dir else None
                        self.model = CLIPModel.from_pretrained(
                            source['path'],
                            cache_dir=cache_dir,
                            local_files_only=False,
                            trust_remote_code=True
                        ).to(self.device)
                        self.processor = CLIPProcessor.from_pretrained(
                            source['path'],
                            cache_dir=cache_dir,
                            local_files_only=False,
                            trust_remote_code=True
                        )

                    print(f"âœ… CLIPæ¨¡å‹åŠ è½½å®Œæˆ! ä½¿ç”¨: {source['name']}")
                    model_loaded = True
                    break

                except Exception as e:
                    print(f"âŒ {source['name']} åŠ è½½å¤±è´¥: {e}")
                    continue

            if not model_loaded:
                raise Exception("æ‰€æœ‰æ¨¡å‹æºéƒ½åŠ è½½å¤±è´¥")

        except Exception as e:
            print(f"CLIPæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise

        # å®šä¹‰ç”¨äºè´¨é‡è¯„ä¼°çš„æ–‡æœ¬æç¤º
        self.quality_prompts = {
            "high_quality": [
                "a complete intact vase viewed from the front",
                "a whole undamaged vase in frontal view",
                "a perfect ceramic vase facing forward",
                "an intact decorative vase front view",
                "a complete flower vase shown frontally",
                "a full unbroken vase from front angle",
                "an entire undamaged pottery vase facing camera",
                "a pristine whole vase in front perspective"
            ],
            "low_quality": [
                "broken vase fragments and pieces",
                "shattered ceramic vase debris",
                "cracked and damaged vase parts",
                "incomplete vase with missing pieces",
                "fragmented broken pottery",
                "ceramic vase shards and chips",
                "damaged vase with cracks and breaks",
                "partial vase pieces and fragments"
            ]
        }

        # ç¼–ç æ–‡æœ¬æç¤º
        self.encode_text_prompts()

    def setup_custom_cache(self, cache_dir):
        """è®¾ç½®è‡ªå®šä¹‰ç¼“å­˜ç›®å½•"""
        cache_path = Path(cache_dir)
        cache_path.mkdir(parents=True, exist_ok=True)

        print(f"è®¾ç½®è‡ªå®šä¹‰ç¼“å­˜ç›®å½•: {cache_dir}")

        # è®¾ç½®ç¯å¢ƒå˜é‡
        os.environ['CLIP_CACHE_DIR'] = str(cache_path)
        os.environ['XDG_CACHE_HOME'] = str(cache_path.parent)

        # åˆ›å»ºclipå­ç›®å½•
        clip_cache_dir = cache_path / "clip"
        clip_cache_dir.mkdir(exist_ok=True)

        # å¦‚æœåŸç¼“å­˜ç›®å½•å­˜åœ¨ä¸”æœ‰æ–‡ä»¶ï¼Œå°è¯•ç§»åŠ¨
        default_cache = Path.home() / ".cache" / "clip"
        if default_cache.exists():
            print(f"æ£€æµ‹åˆ°åŸç¼“å­˜ç›®å½•: {default_cache}")
            try:
                # ç§»åŠ¨ç°æœ‰æ–‡ä»¶åˆ°æ–°ä½ç½®
                for file in default_cache.glob("*"):
                    if file.is_file():
                        target = clip_cache_dir / file.name
                        if not target.exists():
                            shutil.move(str(file), str(target))
                            print(f"ç§»åŠ¨ç¼“å­˜æ–‡ä»¶: {file.name}")
            except Exception as e:
                print(f"ç§»åŠ¨ç¼“å­˜æ–‡ä»¶æ—¶å‡ºé”™: {e}")

        print(f"CLIPæ¨¡å‹å°†ç¼“å­˜åˆ°: {clip_cache_dir}")

    def encode_text_prompts(self):
        """ç¼–ç æ‰€æœ‰æ–‡æœ¬æç¤º"""
        print("ç¼–ç æ–‡æœ¬æç¤º...")

        # ç¼–ç é«˜è´¨é‡æç¤º
        high_quality_inputs = self.processor(text=self.quality_prompts["high_quality"], return_tensors="pt", padding=True)
        high_quality_inputs = {k: v.to(self.device) for k, v in high_quality_inputs.items()}

        with torch.no_grad():
            high_quality_outputs = self.model.get_text_features(**high_quality_inputs)
            self.high_quality_features = high_quality_outputs.mean(dim=0, keepdim=True)
            self.high_quality_features /= self.high_quality_features.norm(dim=-1, keepdim=True)

        # ç¼–ç ä½è´¨é‡æç¤º
        low_quality_inputs = self.processor(text=self.quality_prompts["low_quality"], return_tensors="pt", padding=True)
        low_quality_inputs = {k: v.to(self.device) for k, v in low_quality_inputs.items()}

        with torch.no_grad():
            low_quality_outputs = self.model.get_text_features(**low_quality_inputs)
            self.low_quality_features = low_quality_outputs.mean(dim=0, keepdim=True)
            self.low_quality_features /= self.low_quality_features.norm(dim=-1, keepdim=True)

        print("æ–‡æœ¬æç¤ºç¼–ç å®Œæˆ!")

    def calculate_quality_score(self, image_path):
        """
        è®¡ç®—å›¾åƒçš„è´¨é‡åˆ†æ•°
        è¿”å›: (quality_score, high_quality_sim, low_quality_sim)
        """
        try:
            # åŠ è½½å’Œé¢„å¤„ç†å›¾åƒ
            image = Image.open(image_path).convert('RGB')
            image_inputs = self.processor(images=image, return_tensors="pt")
            image_inputs = {k: v.to(self.device) for k, v in image_inputs.items()}

            # ç¼–ç å›¾åƒ
            with torch.no_grad():
                image_features = self.model.get_image_features(**image_inputs)
                image_features /= image_features.norm(dim=-1, keepdim=True)

                # è®¡ç®—ä¸é«˜è´¨é‡å’Œä½è´¨é‡æç¤ºçš„ç›¸ä¼¼åº¦
                high_quality_sim = torch.cosine_similarity(image_features, self.high_quality_features).item()
                low_quality_sim = torch.cosine_similarity(image_features, self.low_quality_features).item()

                # è´¨é‡åˆ†æ•° = é«˜è´¨é‡ç›¸ä¼¼åº¦ - ä½è´¨é‡ç›¸ä¼¼åº¦
                quality_score = high_quality_sim - low_quality_sim

                return quality_score, high_quality_sim, low_quality_sim

        except Exception as e:
            print(f"å¤„ç†å›¾åƒ {image_path} æ—¶å‡ºé”™: {e}")
            return -1.0, 0.0, 1.0  # è¿”å›ä½è´¨é‡åˆ†æ•°

    def get_image_files(self, input_dir):
        """è·å–ç›®å½•ä¸­çš„æ‰€æœ‰å›¾åƒæ–‡ä»¶"""
        input_path = Path(input_dir)

        # æ”¯æŒçš„å›¾åƒæ ¼å¼
        image_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.webp', '.JPG', '.JPEG', '.PNG', '.BMP', '.TIFF', '.WEBP'}

        # è·å–æ‰€æœ‰å›¾åƒæ–‡ä»¶
        image_files = []
        for file_path in input_path.iterdir():
            if file_path.is_file() and file_path.suffix in image_extensions:
                image_files.append(file_path)

        return image_files

    def filter_images(self, input_dir, output_dir, threshold=0.1, save_rejected=True):
        """
        è¿‡æ»¤å›¾åƒæ•°æ®é›†

        Args:
            input_dir: è¾“å…¥å›¾åƒç›®å½•
            output_dir: è¾“å‡ºç›®å½•
            threshold: è´¨é‡é˜ˆå€¼ï¼Œé«˜äºæ­¤å€¼çš„å›¾åƒè¢«ä¿ç•™
            save_rejected: æ˜¯å¦ä¿å­˜è¢«æ‹’ç»çš„å›¾åƒ
        """
        input_path = Path(input_dir)
        output_path = Path(output_dir)

        # æ£€æŸ¥è¾“å…¥ç›®å½•æ˜¯å¦å­˜åœ¨
        if not input_path.exists():
            raise FileNotFoundError(f"è¾“å…¥ç›®å½•ä¸å­˜åœ¨: {input_dir}")

        # åˆ›å»ºè¾“å‡ºç›®å½•
        accepted_dir = output_path / "accepted"
        rejected_dir = output_path / "rejected"
        accepted_dir.mkdir(parents=True, exist_ok=True)
        if save_rejected:
            rejected_dir.mkdir(parents=True, exist_ok=True)

        # è·å–æ‰€æœ‰å›¾åƒæ–‡ä»¶
        image_files = self.get_image_files(input_dir)

        print(f"åœ¨ç›®å½• {input_dir} ä¸­æ‰¾åˆ° {len(image_files)} ä¸ªå›¾åƒæ–‡ä»¶")

        if len(image_files) == 0:
            print("è­¦å‘Š: æ²¡æœ‰æ‰¾åˆ°ä»»ä½•å›¾åƒæ–‡ä»¶!")
            return None

        # å­˜å‚¨ç»“æœ
        results = []
        accepted_count = 0
        rejected_count = 0

        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = time.time()

        # å¤„ç†æ¯ä¸ªå›¾åƒ
        for i, image_file in enumerate(tqdm(image_files, desc="å¤„ç†å›¾åƒ")):
            quality_score, high_sim, low_sim = self.calculate_quality_score(image_file)

            result = {
                "filename": image_file.name,
                "quality_score": quality_score,
                "high_quality_similarity": high_sim,
                "low_quality_similarity": low_sim,
                "accepted": quality_score >= threshold
            }
            results.append(result)

            # å¤åˆ¶æ–‡ä»¶åˆ°ç›¸åº”ç›®å½•
            try:
                if quality_score >= threshold:
                    shutil.copy2(image_file, accepted_dir / image_file.name)
                    accepted_count += 1
                else:
                    if save_rejected:
                        shutil.copy2(image_file, rejected_dir / image_file.name)
                    rejected_count += 1
            except Exception as e:
                print(f"å¤åˆ¶æ–‡ä»¶ {image_file.name} æ—¶å‡ºé”™: {e}")

            # æ¯å¤„ç†100ä¸ªæ–‡ä»¶æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
            if (i + 1) % 100 == 0:
                elapsed = time.time() - start_time
                avg_time = elapsed / (i + 1)
                remaining = (len(image_files) - i - 1) * avg_time
                print(f"å·²å¤„ç† {i+1}/{len(image_files)} ä¸ªæ–‡ä»¶, é¢„è®¡å‰©ä½™æ—¶é—´: {remaining/60:.1f}åˆ†é’Ÿ")

        # ä¿å­˜ç»“æœæŠ¥å‘Š
        report = {
            "input_directory": str(input_dir),
            "output_directory": str(output_dir),
            "total_images": len(image_files),
            "accepted_images": accepted_count,
            "rejected_images": rejected_count,
            "acceptance_rate": accepted_count / len(image_files) if image_files else 0,
            "threshold": threshold,
            "processing_time_seconds": time.time() - start_time,
            "results": results
        }

        report_file = output_path / "filtering_report.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)

        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        print(f"\n" + "="*60)
        print(f"è¿‡æ»¤å®Œæˆ!")
        print(f"æ€»å›¾åƒæ•°: {len(image_files)}")
        print(f"æ¥å—çš„å›¾åƒ: {accepted_count} ({accepted_count/len(image_files)*100:.1f}%)")
        print(f"æ‹’ç»çš„å›¾åƒ: {rejected_count} ({rejected_count/len(image_files)*100:.1f}%)")
        print(f"è´¨é‡é˜ˆå€¼: {threshold}")
        print(f"å¤„ç†æ—¶é—´: {(time.time() - start_time)/60:.1f}åˆ†é’Ÿ")
        print(f"ç»“æœä¿å­˜åœ¨: {output_path}")
        print(f"è¯¦ç»†æŠ¥å‘Š: {report_file}")
        print("="*60)

        return report

    def analyze_quality_distribution(self, input_dir, sample_size=None):
        """
        åˆ†ææ•°æ®é›†çš„è´¨é‡åˆ†å¸ƒï¼Œå¸®åŠ©ç¡®å®šåˆé€‚çš„é˜ˆå€¼
        """
        input_path = Path(input_dir)

        # æ£€æŸ¥è¾“å…¥ç›®å½•æ˜¯å¦å­˜åœ¨
        if not input_path.exists():
            raise FileNotFoundError(f"è¾“å…¥ç›®å½•ä¸å­˜åœ¨: {input_dir}")

        # è·å–æ‰€æœ‰å›¾åƒæ–‡ä»¶
        image_files = self.get_image_files(input_dir)

        print(f"åœ¨ç›®å½• {input_dir} ä¸­æ‰¾åˆ° {len(image_files)} ä¸ªå›¾åƒæ–‡ä»¶")

        if len(image_files) == 0:
            print("è­¦å‘Š: æ²¡æœ‰æ‰¾åˆ°ä»»ä½•å›¾åƒæ–‡ä»¶!")
            return None, None

        # å¦‚æœæŒ‡å®šäº†æ ·æœ¬å¤§å°ï¼Œéšæœºé‡‡æ ·
        if sample_size and len(image_files) > sample_size:
            import random
            image_files = random.sample(image_files, sample_size)
            print(f"éšæœºé‡‡æ · {len(image_files)} ä¸ªæ–‡ä»¶è¿›è¡Œåˆ†æ")

        quality_scores = []

        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = time.time()

        for i, image_file in enumerate(tqdm(image_files, desc="åˆ†æè´¨é‡")):
            quality_score, _, _ = self.calculate_quality_score(image_file)
            quality_scores.append(quality_score)

            # æ¯å¤„ç†50ä¸ªæ–‡ä»¶æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
            if (i + 1) % 50 == 0:
                elapsed = time.time() - start_time
                avg_time = elapsed / (i + 1)
                remaining = (len(image_files) - i - 1) * avg_time
                print(f"å·²åˆ†æ {i+1}/{len(image_files)} ä¸ªæ–‡ä»¶, é¢„è®¡å‰©ä½™æ—¶é—´: {remaining/60:.1f}åˆ†é’Ÿ")

        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        quality_scores = np.array(quality_scores)
        stats = {
            "mean": float(np.mean(quality_scores)),
            "std": float(np.std(quality_scores)),
            "min": float(np.min(quality_scores)),
            "max": float(np.max(quality_scores)),
            "median": float(np.median(quality_scores)),
            "percentile_10": float(np.percentile(quality_scores, 10)),
            "percentile_25": float(np.percentile(quality_scores, 25)),
            "percentile_50": float(np.percentile(quality_scores, 50)),
            "percentile_75": float(np.percentile(quality_scores, 75)),
            "percentile_90": float(np.percentile(quality_scores, 90))
        }

        print(f"\n" + "="*60)
        print(f"è´¨é‡åˆ†æ•°ç»Ÿè®¡ (åŸºäº {len(image_files)} ä¸ªæ ·æœ¬):")
        print(f"å¹³å‡å€¼: {stats['mean']:.3f}")
        print(f"æ ‡å‡†å·®: {stats['std']:.3f}")
        print(f"æœ€å°å€¼: {stats['min']:.3f}")
        print(f"æœ€å¤§å€¼: {stats['max']:.3f}")
        print(f"ä¸­ä½æ•°: {stats['median']:.3f}")
        print(f"10%åˆ†ä½æ•°: {stats['percentile_10']:.3f}")
        print(f"25%åˆ†ä½æ•°: {stats['percentile_25']:.3f}")
        print(f"75%åˆ†ä½æ•°: {stats['percentile_75']:.3f}")
        print(f"90%åˆ†ä½æ•°: {stats['percentile_90']:.3f}")

        # å»ºè®®ä¸åŒçš„é˜ˆå€¼ç­–ç•¥
        print(f"\né˜ˆå€¼å»ºè®®:")
        print(f"ä¿å®ˆè¿‡æ»¤ (ä¿ç•™90%): {stats['percentile_10']:.3f}")
        print(f"ä¸­ç­‰è¿‡æ»¤ (ä¿ç•™75%): {stats['percentile_25']:.3f}")
        print(f"å¹³è¡¡è¿‡æ»¤ (ä¿ç•™50%): {stats['percentile_50']:.3f}")
        print(f"æ¿€è¿›è¿‡æ»¤ (ä¿ç•™25%): {stats['percentile_75']:.3f}")
        print(f"ä¸¥æ ¼è¿‡æ»¤ (ä¿ç•™10%): {stats['percentile_90']:.3f}")
        print("="*60)

        return stats, quality_scores

def select_device():
    """äº¤äº’å¼é€‰æ‹©è®¾å¤‡"""
    if not torch.cuda.is_available():
        print("âš ï¸  CUDAä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CPU")
        return "cpu"

    gpu_count = torch.cuda.device_count()
    print(f"\nğŸ® æ£€æµ‹åˆ° {gpu_count} ä¸ªGPU:")

    for i in range(gpu_count):
        gpu_name = torch.cuda.get_device_name(i)
        gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3
        print(f"  GPU {i}: {gpu_name} ({gpu_memory:.1f} GB)")

    print(f"  CPU: ä½¿ç”¨CPUæ¨¡å¼")
    print("")

    while True:
        choice = input(f"è¯·é€‰æ‹©è®¾å¤‡ (0-{gpu_count-1} é€‰æ‹©GPU, c é€‰æ‹©CPU): ").strip().lower()

        if choice == 'c':
            return "cpu"
        elif choice.isdigit():
            gpu_id = int(choice)
            if 0 <= gpu_id < gpu_count:
                return f"cuda:{gpu_id}"
            else:
                print(f"âŒ æ— æ•ˆçš„GPU IDï¼Œè¯·è¾“å…¥ 0-{gpu_count-1}")
        else:
            print("âŒ æ— æ•ˆè¾“å…¥ï¼Œè¯·è¾“å…¥æ•°å­—æˆ– 'c'")

def main():
    # é»˜è®¤è·¯å¾„
    default_input_dir = "./images3/1"
    default_output_dir = "./filtered_vases"

    parser = argparse.ArgumentParser(description="ä½¿ç”¨CLIPè¿‡æ»¤èŠ±ç“¶å›¾åƒæ•°æ®é›†")
    parser.add_argument("--input_dir", type=str, default=default_input_dir,
                       help=f"è¾“å…¥å›¾åƒç›®å½•è·¯å¾„ (é»˜è®¤: {default_input_dir})")
    parser.add_argument("--output_dir", type=str, default=default_output_dir,
                       help=f"è¾“å‡ºç›®å½•è·¯å¾„ (é»˜è®¤: {default_output_dir})")
    parser.add_argument("--threshold", type=float, default=None,
                       help="è´¨é‡é˜ˆå€¼ (å¦‚æœä¸æŒ‡å®šï¼Œä¼šå…ˆåˆ†ææ•°æ®é›†)")
    parser.add_argument("--analyze_only", action="store_true",
                       help="ä»…åˆ†æè´¨é‡åˆ†å¸ƒï¼Œä¸è¿›è¡Œè¿‡æ»¤")
    parser.add_argument("--sample_size", type=int, default=500,
                       help="åˆ†ææ—¶çš„é‡‡æ ·å¤§å° (é»˜è®¤: 500)")
    parser.add_argument("--device", type=str, default=None,
                       help="æŒ‡å®šè®¾å¤‡ (cuda:0, cuda:1, cpu ç­‰)")
    parser.add_argument("--model_cache_dir", type=str,
                       default="./CLIP/model",
                       help="CLIPæ¨¡å‹ç¼“å­˜ç›®å½•")
    parser.add_argument("--auto_select_device", action="store_true",
                       help="äº¤äº’å¼é€‰æ‹©è®¾å¤‡")

    args = parser.parse_args()

    print("="*60)
    print("CLIP èŠ±ç“¶å›¾åƒè´¨é‡è¿‡æ»¤å™¨")
    print("="*60)
    print(f"è¾“å…¥ç›®å½•: {args.input_dir}")
    print(f"è¾“å‡ºç›®å½•: {args.output_dir}")

    # æ£€æŸ¥è¾“å…¥ç›®å½•æ˜¯å¦å­˜åœ¨
    if not os.path.exists(args.input_dir):
        print(f"é”™è¯¯: è¾“å…¥ç›®å½• '{args.input_dir}' ä¸å­˜åœ¨")
        print("è¯·ç¡®è®¤è·¯å¾„æ˜¯å¦æ­£ç¡®")
        return

    # è®¾å¤‡é€‰æ‹©
    device = args.device
    if args.auto_select_device or device is None:
        device = select_device()

    # åˆå§‹åŒ–è¿‡æ»¤å™¨
    try:
        filter_tool = ServerVaseImageFilter(device=device, model_cache_dir=args.model_cache_dir)
    except Exception as e:
        print(f"åˆå§‹åŒ–CLIPæ¨¡å‹å¤±è´¥: {e}")
        print("è¯·ç¡®ä¿å·²å®‰è£…å¿…è¦çš„ä¾èµ–åŒ…:")
        print("pip install torch torchvision transformers Pillow numpy tqdm")
        return

    # å¦‚æœåªæ˜¯åˆ†ææ¨¡å¼
    if args.analyze_only:
        try:
            stats, scores = filter_tool.analyze_quality_distribution(
                args.input_dir, args.sample_size
            )
            if stats is None:
                return
        except Exception as e:
            print(f"åˆ†æè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        return

    # å¦‚æœæ²¡æœ‰æŒ‡å®šé˜ˆå€¼ï¼Œå…ˆåˆ†ææ•°æ®é›†
    if args.threshold is None:
        print("æœªæŒ‡å®šé˜ˆå€¼ï¼Œå…ˆåˆ†ææ•°æ®é›†è´¨é‡åˆ†å¸ƒ...")
        try:
            stats, scores = filter_tool.analyze_quality_distribution(
                args.input_dir, args.sample_size
            )
            if stats is None:
                return

            # ä½¿ç”¨25%åˆ†ä½æ•°ä½œä¸ºé»˜è®¤é˜ˆå€¼ï¼ˆä¿ç•™75%çš„æ•°æ®ï¼‰
            threshold = stats['percentile_25']
            print(f"\nä½¿ç”¨å»ºè®®é˜ˆå€¼: {threshold:.3f} (ä¿ç•™çº¦75%çš„æ•°æ®)")

            # è¯¢é—®ç”¨æˆ·æ˜¯å¦ç»§ç»­
            user_input = input("\næ˜¯å¦ä½¿ç”¨æ­¤é˜ˆå€¼ç»§ç»­è¿‡æ»¤? (y/n/è‡ªå®šä¹‰é˜ˆå€¼): ").strip().lower()
            if user_input == 'n':
                print("è¿‡æ»¤å·²å–æ¶ˆ")
                return
            elif user_input != 'y':
                try:
                    threshold = float(user_input)
                    print(f"ä½¿ç”¨è‡ªå®šä¹‰é˜ˆå€¼: {threshold}")
                except ValueError:
                    print("æ— æ•ˆè¾“å…¥ï¼Œä½¿ç”¨é»˜è®¤é˜ˆå€¼")

        except Exception as e:
            print(f"åˆ†æè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            return
    else:
        threshold = args.threshold

    # æ‰§è¡Œè¿‡æ»¤
    try:
        print(f"\nå¼€å§‹è¿‡æ»¤ï¼Œé˜ˆå€¼: {threshold}")
        report = filter_tool.filter_images(
            args.input_dir,
            args.output_dir,
            threshold=threshold
        )

        if report:
            print(f"\nè¿‡æ»¤å®Œæˆ! è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {args.output_dir}/filtering_report.json")

    except Exception as e:
        print(f"è¿‡æ»¤è¿‡ç¨‹ä¸­å‡ºé”™: {e}")

if __name__ == "__main__":
    main()
